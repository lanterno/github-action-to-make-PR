projects:
  secvm:
    name: SecVM
    categories:
      - Learning
      - Privacy
    applications:
      - Infra
    type: Experiments
    description: Privacy-preserving classification
    layman_desc: >
      Today, large amounts of valuable data are distributed among millions of
      user-held devices, such as personal computers, phones, or
      Internet-of-things devices. Many companies collect such data with the goal
      of using it for training machine learning models allowingthem to improve
      their services. User-held data is, however, often sensitive, and
      collecting it is problematic in terms of privacy.  We propose a novel way
      of training a supervised classifier in a distributed setting akin to the
      recently proposed federated learning paradigm, but under the stricter
      privacy requirement that the server that trains the model is assumed to be
      untrusted and potentially malicious. We thus preserve user privacy by
      design, rather than by trust.
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/secvm-server
      date_last_commit: 2020-08-16
    language: Java
    tags:
      - Decentralized
      - Distributed Learning
    information:
      - type: Paper
        title: Privacy-Preserving Classification with Secret Vector Machines
        url: https://arxiv.org/pdf/1907.03373.pdf
      - type: Paper
        title: Privacy-Preserving Distributed Learning with Secret Gradient Descent
        url: https://arxiv.org/pdf/1906.11993.pdf
    date_added: 2021-11-05
    date_updated: 2024-04-16
    maturity: 1
  invariant-language-models:
    name: Invariant Language Modeling
    categories:
      - Learning
    applications:
      - Info
    type: Application
    description: Invariant natural language modeling
    layman_desc: >
      Modern pretrained language models are critical components for natural
      language processing. Yet, they suffer from spurious correlations, poor
      out-of-domain generalization, and biases. This is a framework to learn
      invariant representations that should generalize across training
      environments.
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/invariant-language-models
      date_last_commit: 2022-01-24
    language: Python
    license: Apache-2.0
    tags:
      - Natural Language
    date_added: 2021-11-05
    date_updated: 2024-04-16
  eighenthemes:
    name: Eigenthemes
    categories:
      - Learning
    applications:
      - Info
    type: Experiments
    description: Improved entity linking
    layman_desc: >
      In natural language processing, entity linking, i.e. the task of assigning
      a unique identity to entities (for example "Paris" in a sentence refers to
      the city, not to someone's name), is an important problem. Most previous
      solutions rely on annotated data, which is however not available in many
      domains. We propose a method for entity linking without the need for
      annotated data.
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/eigenthemes
      date_last_commit: 2021-09-23
    language: Python
    license: Apache-2.0
    tags:
      - Natural Language
    information:
      - type: Paper
        title: Low-Rank Subspaces for Unsupervised Entity Linking
        url: https://arxiv.org/pdf/2104.08737.pdf
        notes:
          - label: Published at
            text: EMNLP 2021
            url: https://2021.emnlp.org/
    date_added: 2021-11-05
    date_updated: 2024-04-16
  quotebank:
    name: Quotebank
    categories:
      - Learning
    applications:
      - Info
    type: Experiments
    description: Corpus of quotations from a decade of news
    layman_desc: >
      News from half a million of website, over the last 15 years, labelled by
      writer. Quotebank shows that it can accurately associate any citation with
      who most probably wrote it. It helps in identifying the source of a given
      news without any metadata.
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/Quotebank
      date_last_commit: 2021-07-23
    language: Python
    license: MIT
    tags:
      - Natural Language
    information:
      - type: Paper
        title: "Quotebank: A Corpus of Quotations from a Decade of News"
        url: https://infoscience.epfl.ch/record/294900
        notes:
          - label: Published at
            text: WSDM 2021
            url: https://www.wsdm-conference.org/2021/proceedings.php
      - type: Demo
        title: Quotebank
        url: https://quotebank.dlab.tools
    date_added: 2022-09-28
    date_updated: 2024-04-16
  dipps:
    name: DiPPS
    categories:
      - Privacy
      - Learning
    applications:
      - Info
    type: Experiments
    tags:
      - Survey
    description: Differentially Private Propensity Scores for Bias Correction
    layman_desc: >
      In surveys, it is typically up to the individuals to decide if they want
      to participate or not, which leads to participation bias: the individuals
      willing to share their data might not be representative of the entire
      population. Similarly, there are cases where one does not have direct
      access to any data of the target population and has to resort to publicly
      available proxy data sampled from a different distribution. In this paper,
      we present Differentially Private Propensity Scores for Bias Correction
      (DiPPS), a method for approximating the true data distribution of interest
      in both of the above settings
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/DIPPS
      date_last_commit: 2022-10-06
    language: Python
    license: other
    information:
      - type: Paper
        title: Differentially Private Propensity Scores for Bias Correction
        url: https://arxiv.org/abs/2210.02360
    date_added: 2023-03-16
    date_updated: 2024-04-16
  genie:
    name: GenIE
    categories:
      - Privacy
      - Learning
    applications:
      - Info
    type: Experiments
    tags:
      - Natural Language
    description: Autoregressive information extraction system
    layman_desc: >
      GenIE uses a sequence-to-sequence model that takes unstructured text as
      input and autoregressively generates a structured semantic representation
      of the information expressed in it, in the form of (subject, relation,
      object) triplets, as output.
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/GenIE
      date_last_commit: 2023-03-28
    language: Python
    license: MIT
    information:
      - type: Paper
        title: "GenIE: Generative Information Extraction"
        url: https://arxiv.org/abs/2112.08340
    date_added: 2023-03-16
    date_updated: 2024-04-16
  synthie:
    name: SynthIE
    categories:
      - Learning
    applications:
      - Info
    type: Experiments
    tags:
      - Natural Language
    description: Exploiting Asymmetry for Synthetic Training Data Generation
    code:
      type: Lab GitHub
      url: https://github.com/epfl-dlab/SynthIE
      date_last_commit: 2023-05-27
    language: Python
    license: MIT
    information:
      - type: Paper
        title: "Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and
          the Case of Information Extraction"
        url: https://arxiv.org/abs/2303.04132
    date_added: 2023-03-16
    date_updated: 2024-04-16
  aiflows:
    name: aiFlows
    description: |
      Modular AI collaboration framework
    type: Framework
    categories:
      - Learning
    applications:
      - Info
    tags:
      - Machine Learning
      - Cloud
      - Protocol
    layman_desc: >
      aiFlows simplifies the design and implementation of complex workflows
      involving humans,  AI systems, and tools.   It enables modularity by
      allowing Flows to be stacked like LEGO blocks, reusability by  sharing
      Flows on the FlowVerse, remote peer-to-peer collaboration between Flows,
      and  concurrent execution of multiple Flows.   The goal is to empower
      researchers and practitioners with complete control and  customizability
      over their AI workflows.
    tech_desc: >
      aiFlows is a framework centered around Flows and messages.   Flows are
      independent, self-contained computational building blocks that can
      complete  semantically meaningful units of work.   Flows communicate via a
      standardized message-based interface, enabling modularity,  reusability,
      remote peer-to-peer collaboration, and concurrency.
    url: https://epfl-dlab.github.io/aiflows/docs/built_with_sphinx/html/index.html
    code:
      type: Lab Github
      url: https://github.com/epfl-dlab/aiflows?tab=readme-ov-file
      date_last_commit: 2024-04-12
    language: Python
    license: MIT
    date_added: 2024-04-16
  transformers-cfg:
    name: Transformers CFG
    description: |
      Grammar-constrained text generation with Transformers models
    type: Library
    categories:
      - Learning
    applications:
      - Info
    tags:
      - Machine Learning
      - Natural Language
    layman_desc: >
      The transformers_cfg library allows you to control the output of language
      models like GPT-3 by providing a set of rules (grammar) that the generated
      text must follow.   This is useful for generating structured data like
      code, JSON objects, or any text that needs to conform to specific patterns
      or rules.   The library works with popular language models and provides an
      easy way to incorporate grammar constraints into the text generation
      process without modifying the underlying models.
    tech_desc: >
      Transformers_cfg is an extension library for the Hugging Face Transformers
      library that enables grammar-constrained text generation.   It provides
      tools and functionalities to work with context-free grammars (CFGs) for
      natural language processing tasks involving CFGs.   The library supports
      various Transformer models, including LLaMa, GPT, Bloom, Mistral, and
      Falcon, and offers features like multilingual grammar support and
      integration with Text-Generation-WebUI.
    code:
      type: Lab Github
      url: https://github.com/epfl-dlab/transformers-CFG
      date_last_commit: 2025-04-10
    language: Python
    license: MIT
    date_added: 2024-04-16
  multilingual-entity-insertion:
    name: Entity Insertion in Wikipedia
    description: |
      Multilingual entity insertion in Wikipedia articles
    type: Experiments
    categories:
      - Learning
    applications:
      - Info
    tags:
      - Machine Learning
      - Natural Language
    layman_desc: >
      Automatically adding relevant links to entities in Wikipedia articles
      across different languages is a challenging task. This project provides a
      solution by processing data from Wikipedia dumps and training machine
      learning models. The data processing extracts information like articles,
      links, and mentions from the dumps. The modeling code trains models to
      rank candidate text spans for inserting an entity link. The models are
      evaluated against various baselines like keyword matching and language
      models. This helps in improving the quality and consistency of Wikipedia
      by suggesting relevant entity links across multiple languages.
    tech_desc: >
      Proposes a framework for inserting entities into Wikipedia articles across
      multiple languages. It processes Wikipedia dumps to extract data and train
      models for entity insertion. The key components are: 1) Data processing
      pipeline to extract relevant data from Wikipedia dumps. 2) Modeling code
      for training entity insertion models using a ranking loss or pointwise
      loss. 3) Benchmarking code to evaluate models against baselines like BM25,
      EntQA, and GPT language models.
    code:
      type: Lab Github
      url: https://github.com/epfl-dlab/multilingual-entity-insertion
      date_last_commit: 2024-10-04
    language: Jupyter Notebook
    date_added: 2024-04-16
  refiner:
    name: Refiner
    description: |
      Reasoning framework with feedback on intermediate steps
    type: Toolset
    categories:
      - Learning
    applications:
      - Info
    tags:
      - Machine Learning
      - Natural Language
      - Optimization
    layman_desc: >
      This project introduces REFINER, a system that helps language models
      improve their reasoning abilities through feedback.  It has one model that
      generates initial reasoning steps, and another model that critiques those
      steps.  By getting feedback on the intermediate reasoning, the first model
      can refine its final answer.  This allows language models to solve complex
      reasoning tasks more accurately.
    tech_desc: >
      REFINER is an interaction-based framework for natural language reasoning
      tasks.  It has a CRITIC model that provides structured feedback on
      intermediate reasoning steps, and a GENERATOR model that solves the
      reasoning task by first generating intermediate steps.  The core idea is
      the interaction between the generator and critic, where the generator's
      steps are improved via feedback from the critic.
    code:
      type: Personal GitHub
      url: https://github.com/debjitpaul/refiner
      date_last_commit: 2024-02-20
    language: Python
    license: Apache-2.0
    date_added: 2024-05-03
  lamen:
    name: LAMEN
    description: |
      Evaluating language models through negotiation tasks
    type: Toolset
    categories:
      - Learning
    applications:
      - Info
    tags:
      - Machine Learning
      - Natural Language
      - Optimization
    layman_desc: >
      This project introduces a new way to test the decision-making abilities of
      AI language models by having them engage in negotiations with each other.
      By designing various negotiation scenarios, such as dividing pizza slices
      or deciding on the amount of cheese, the researchers can evaluate how well
      the AI models perform in terms of reaching agreements, maximizing their
      own benefits, and cooperating when necessary. The study also examines how
      faithfully the AI models follow their own reasoning and instructions,
      providing insights into their reliability and alignment with human values.
    tech_desc: >
      The project proposes using structured negotiations as a dynamic benchmark
      for evaluating language model (LM) agents. The negotiation framework
      consists of a game setting, issues to negotiate, and optional preference
      weights, allowing for the design of complex games by increasing the number
      of issues, mixing issue types, and adding non-uniform preferences. The
      benchmark setup jointly evaluates performance metrics (utility and
      completion rate) and alignment metrics (faithfulness and
      instruction-following) in self-play and cross-play settings.
    url: https://dlab.epfl.ch/2024-01-10-evaluating-language-model-agency/
    code:
      type: Lab Github
      url: https://github.com/epfl-dlab/LAMEN
      date_last_commit: 2024-05-03
    language: Python
    date_added: 2024-05-03
  llm-ga:
    name: LLM Grounding Analysis
    description: |
      LLM grounding vs. factual recall
    type: Experiments
    categories:
      - Learning
    applications:
      - Gov
    tags:
      - Machine Learning
      - Natural Language
    layman_desc: >
      Large language models (LLMs) can memorize and apply new information.
      However, it's unclear how they balance this new context with their
      pre-existing knowledge. This research analyzes how LLMs manage this
      conflict using a new counterfactual dataset.
    tech_desc: >
      The study investigates LLMs using Fakepedia, a dataset presenting
      contradictions  between known facts and new information. Through Masked
      Grouped Causal Tracing (MGCT), the research deciphers LLMs' grounding
      mechanisms by contrasting neural activation patterns. Findings help
      understand the co-functioning of grounding with recall capabilities within
      LLMs.
    code:
      type: Lab Github
      url: https://github.com/epfl-dlab/llm-grounding-analysis
      date_last_commit: 2024-02-19
    language: Python
    license: Apache-2.0
    date_added: 2024-05-03
